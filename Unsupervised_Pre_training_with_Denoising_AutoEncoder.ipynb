{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Pre-training with Denoising AutoEncoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNg+5R+w61ImYOdsvpTCsuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugeneLogvinovsky/ITHillel/blob/main/Unsupervised_Pre_training_with_Denoising_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sshC-YcxQ_G"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "from torchvision.datasets import STL10\r\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose, RandomHorizontalFlip, RandomCrop\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "from torch import nn, optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXyaid6BxVAl",
        "outputId": "9968d3ed-f5f2-4329-d347-4d9bc4f2ee64"
      },
      "source": [
        "train_ds = STL10('.', split='train', folds=None, transform=ToTensor(), download=True)\r\n",
        "X = torch.stack([train_ds[i][0] for i in range(len(train_ds))], 1).reshape(3, -1)\r\n",
        "stl10_mean, stl10_std = X.mean(1), X.std(1)\r\n",
        "\r\n",
        "train_transforms = Compose([\r\n",
        "    RandomHorizontalFlip(),\r\n",
        "    RandomCrop(size=32, padding=2),\r\n",
        "    ToTensor(),\r\n",
        "    Normalize(mean=stl10_mean, std=stl10_std)])\r\n",
        "train_ds = STL10('.', split='train', folds=None, transform=train_transforms, download=True)\r\n",
        "train_dl = DataLoader(train_ds, batch_size=512, shuffle=True)\r\n",
        "\r\n",
        "val_transforms = Compose([ToTensor(), Normalize(mean=stl10_mean, std=stl10_std)])\r\n",
        "val_ds = STL10('.', split='test', folds=None, transform=val_transforms, download=True)\r\n",
        "val_dl = DataLoader(val_ds, batch_size=1024)\r\n",
        "\r\n",
        "pretrain_ds = STL10('.', split='unlabeled', folds=None, transform=train_transforms, download=True)\r\n",
        "pretrain_dl = DataLoader(pretrain_ds, batch_size=512, shuffle=True)\r\n",
        "\r\n",
        "print(f'Dataset size: train={len(train_ds)}, val={len(val_ds)}, pretrain={len(pretrain_ds)}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset size: train=5000, val=8000, pretrain=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZK_WI65xaKt"
      },
      "source": [
        "# 3072 -> 256 (C=12)\r\n",
        "encoder = nn.Sequential(\r\n",
        "    # 3x32x32 = 3072\r\n",
        "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(32),\r\n",
        "    nn.MaxPool2d(2, 2),\r\n",
        "\r\n",
        "    # 32x16x16\r\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(64),\r\n",
        "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(64),\r\n",
        "    nn.MaxPool2d(2, 2),\r\n",
        "\r\n",
        "    # 64x8x8\r\n",
        "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(128),\r\n",
        "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(128),\r\n",
        "    nn.MaxPool2d(2, 2),\r\n",
        "\r\n",
        "    # 128x4x4\r\n",
        "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(128),\r\n",
        "    nn.Conv2d(128, 64, kernel_size=3, padding=1),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.BatchNorm2d(64),\r\n",
        "    nn.AvgPool2d(2, 2),\r\n",
        "\r\n",
        "    # 64x2x2\r\n",
        "    # nn.Flatten()\r\n",
        "    # 64*2*2 = 256\r\n",
        ")\r\n",
        "\r\n",
        "decoder = nn.Sequential(\r\n",
        "    # 64x2x2\r\n",
        "    nn.ConvTranspose2d(64, 128, kernel_size=3, stride=2),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2),\r\n",
        "    nn.ReLU(),   \r\n",
        "\r\n",
        "    # 128x4x4\r\n",
        "    nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2),\r\n",
        "    nn.ReLU(),\r\n",
        "    # 64x8x8\r\n",
        "    nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2),\r\n",
        "    nn.ReLU(),\r\n",
        "    # 32x16x16\r\n",
        "    nn.Conv2d(32, 3, kernel_size=3, padding=1),\r\n",
        "    nn.Sigmoid()\r\n",
        "    # 3x32x32\r\n",
        ")\r\n",
        "\r\n",
        "model = nn.Sequential(encoder, decoder)\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "#loss = loss_fn(model(X), X)\r\n",
        "#loss.backward()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VYTvo0p5GCE"
      },
      "source": [
        "def define_cnn_extractor():\r\n",
        "    return nn.Sequential(\r\n",
        "        model\r\n",
        "    )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is39X8Af5Oxr"
      },
      "source": [
        "def train(train_dl, val_dl, forward, opt):\r\n",
        "    loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    for epoch_ind in range(10):\r\n",
        "        train_loss, train_acc = [], []\r\n",
        "        for (X, Y) in pretrain_dl:\r\n",
        "            opt.zero_grad()\r\n",
        "            y_pred = forward(X)\r\n",
        "            loss = loss_fn(y_pred, Y)\r\n",
        "            loss.backward()\r\n",
        "            opt.step()\r\n",
        "            train_loss.append(loss.cpu().detach())\r\n",
        "            train_acc.append(torch.mean((y_pred.argmax(1) == Y).float()).item())\r\n",
        "\r\n",
        "        print(f\"T[{epoch_ind}]: loss={np.mean(pretrain_loss):.6f}\\tacc={np.mean(pretrain_acc):.4f}\")\r\n",
        "\r\n",
        "        val_loss, val_acc = [], []\r\n",
        "        for (X, Y) in val_dl:\r\n",
        "            y_pred = forward(X)\r\n",
        "            loss = loss_fn(y_pred, Y)\r\n",
        "            val_loss.append(loss.cpu().detach())\r\n",
        "            val_acc.append(torch.mean((y_pred.argmax(1) == Y).float()).item())\r\n",
        "\r\n",
        "        print(f\"V[{epoch_ind}]: loss={np.mean(val_loss):.6f}\\tacc={np.mean(val_acc):.4f}\\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgW9xdnx53ka"
      },
      "source": [
        "# calculate STL10 mean and std\r\n",
        "\r\n",
        "X = torch.stack([train_ds[i][0] for i in range(len(train_ds))], 1).reshape(3, -1)\r\n",
        "stl10_mean, stl10_std = X.mean(1), X.std(1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qfiuXlC6RfC",
        "outputId": "53a8ad7e-3e9f-4bae-8e4a-c8649df0e94d"
      },
      "source": [
        "# define STL10 model and train it from scratch (pretrain)\r\n",
        "CNN_extractor = define_cnn_extractor()\r\n",
        "STL10_classifier = nn.Linear(256, 10)\r\n",
        "STL10_model = nn.Sequential(CNN_extractor, STL10_classifier)\r\n",
        "print('Train STL10 model from scratch (unlabeled)')\r\n",
        "opt = optim.Adam(params=STL10_model.parameters(), lr=1e-3)\r\n",
        "train(pretrain_dl, val_dl, STL10_model, opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train STL10 model from scratch (unlabeled)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3_N5xat69Mp"
      },
      "source": [
        "# define STL10 model with pre-trained cnn extractor\r\n",
        "#CNN_extractor = define_cnn_extractor()\r\n",
        "STL10_classifier = nn.Linear(256, 100)\r\n",
        "STL10_model = nn.Sequential(CNN_extractor, STL10_classifier)\r\n",
        "print('Train STL10 model pre-trained model (labeled)')\r\n",
        "opt = optim.Adam(params=STL10_model.parameters(), lr=1e-3)\r\n",
        "train(train_dl, val_dl, STL10_model, opt)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}